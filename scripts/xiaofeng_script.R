require(tidyverse)
library(tidymodels)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
library(pROC)

# function to parse html and clean text
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('h1, h2, h3, h4, h5, h6, p') %>%
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}

load("./data/claims-raw.RData")

# will take a couple minutes
claims_clean <- claims_raw %>%
  parse_data()

## PCA
claims_nlp <- nlp_fn(claims_clean)

set.seed(123)
data_split <- initial_split(claims_nlp, prop = 0.7, strata = bclass)
train_data <- training(data_split)
test_data <- testing(data_split)

X_train_tfidf <- train_data %>% 
  select(-c(.id, bclass)) %>% # exclude non-numeric column
  as.matrix() %>% 
  na.omit()
X_test_tfidf <- test_data %>% 
  select(-c(.id, bclass)) %>% 
  as.matrix() %>% 
  na.omit()

common_columns <- intersect(colnames(X_train_tfidf), colnames(X_test_tfidf))
X_train_tfidf <- X_train_tfidf[, common_columns]
X_test_tfidf <- X_test_tfidf[, common_columns]

train_data$bclass <- factor(train_data$bclass, levels = c("N/A: No relevant content.", "Relevant claim content"))
train_data$bclass <- as.numeric(train_data$bclass) - 1  # Convert to 0 and 1

test_data$bclass <- factor(test_data$bclass, levels = c("N/A: No relevant content.", "Relevant claim content"))
test_data$bclass <- as.numeric(test_data$bclass) - 1 

# training data
X_train_scaled <- X_train_tfidf[, apply(X_train_tfidf, 2, var) != 0]

# PCA transformation
pca <- prcomp(X_train_scaled, center = T, scale. = T)

# Calculate cumulative variance and select the number of components
explained_variance <- summary(pca)$importance[2, ]
cumulative_variance <- cumsum(explained_variance)
num_components <- which(cumulative_variance >= 0.90)[1]

X_train_pca <- data.frame(pca$x[, 1:num_components])
X_train_pca$bclass <- train_data$bclass

# will take a couple minutes
model <- glm(bclass ~ ., data = X_train_pca, family = binomial)
summary(model)

# also apply PCA on testing
X_test_pca <- predict(pca, X_test_tfidf)[, 1:num_components]
X_test_pca <- data.frame(X_test_pca)
X_test_pca$bclass <- test_data$bclass

pred_test <- predict(model, X_test_pca, type = "response")
pred_bin_test <- ifelse(pred_test > 0.5, 1, 0)
conf_matrix_test <- table(Predicted = pred_bin_test, Actual = X_test_pca$bclass)
auc_test <- roc(X_test_pca$bclass, pred_test, levels = c(0, 1), direction = "<")$auc


#### without header
parse_fn_wo <- function(.html){
  read_html(.html) %>%
    html_elements('p') %>%
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

# function to apply to claims data
parse_data_wo <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn_wo(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn <- function(parse_data_wo.out){
  out <- parse_data_wo.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}

load("./data/claims-raw.RData")

# will take a couple minutes
claims_clean_wo <- claims_raw %>%
  parse_data_wo()

## PCA
claims_nlp_wo <- nlp_fn(claims_clean_wo)

set.seed(123)
data_split_wo <- initial_split(claims_nlp_wo, prop = 0.7, strata = bclass)
train_data_wo <- training(data_split_wo)
test_data_wo <- testing(data_split_wo)

X_train_tfidf_wo <- train_data_wo %>% 
  select(-c(.id, bclass)) %>% # exclude non-numeric column
  as.matrix() %>% 
  na.omit()
X_test_tfidf_wo <- test_data_wo %>% 
  select(-c(.id, bclass)) %>% 
  as.matrix() %>% 
  na.omit()

common_columns_wo <- intersect(colnames(X_train_tfidf_wo), colnames(X_test_tfidf_wo))
X_train_tfidf_wo <- X_train_tfidf_wo[, common_columns_wo]
X_test_tfidf_wo <- X_test_tfidf_wo[, common_columns_wo]

train_data_wo$bclass <- factor(train_data_wo$bclass, levels = c("N/A: No relevant content.", "Relevant claim content"))
train_data_wo$bclass <- as.numeric(train_data_wo$bclass) - 1  # Convert to 0 and 1

test_data_wo$bclass <- factor(test_data_wo$bclass, levels = c("N/A: No relevant content.", "Relevant claim content"))
test_data_wo$bclass <- as.numeric(test_data_wo$bclass) - 1 

# training data
X_train_scaled_wo <- X_train_tfidf_wo[, apply(X_train_tfidf_wo, 2, var) != 0]

# PCA transformation
pca_wo <- prcomp(X_train_scaled_wo, center = T, scale. = T)

# Calculate cumulative variance and select the number of components
explained_variance_wo <- summary(pca_wo)$importance[2, ]
cumulative_variance_wo <- cumsum(explained_variance_wo)
num_components_wo <- which(cumulative_variance_wo >= 0.90)[1]

X_train_pca_wo <- data.frame(pca_wo$x[, 1:num_components_wo])
X_train_pca_wo$bclass <- train_data_wo$bclass

# will take a couple minutes
model_wo <- glm(bclass ~ ., data = X_train_pca_wo, family = binomial)
summary(model_wo)

# also apply PCA on testing
X_test_pca_wo <- predict(pca_wo, X_test_tfidf_wo)[, 1:num_components_wo]
X_test_pca_wo <- data.frame(X_test_pca_wo)
X_test_pca_wo$bclass <- test_data_wo$bclass

pred_test_wo <- predict(model_wo, X_test_pca_wo, type = "response")
pred_bin_test_wo <- ifelse(pred_test_wo > 0.5, 1, 0)
conf_matrix_test_wo <- table(Predicted = pred_bin_test_wo, Actual = X_test_pca_wo$bclass)
auc_test_wo <- roc(X_test_pca_wo$bclass, pred_test_wo, levels = c(0, 1), direction = "<")$auc
